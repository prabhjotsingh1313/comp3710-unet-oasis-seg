import torch, sys, os, glob, numpy as np, matplotlib.pyplot as plt
print('Torch:', torch.__version__, 'CUDA?', torch.cuda.is_available())

import os, glob
import numpy as np
from PIL import Image
import torch
from torch.utils.data import Dataset
import torch.nn.functional as F

IMG_SIZE = 256  # native size is 256x256; keep it to avoid interpolation noise

def normalize01(x):
    x = x.astype(np.float32)
    if x.max() > 1: x /= 255.0
    return x

class OasisSegDataset(Dataset):
    """
    Images: keras_png_slices_{split}/case_###_slice_#.nii.png
    Masks : keras_png_slices_seg_{split}/seg_ ###_slice_#.nii.png
    """
    def __init__(self, split, root, img_size=IMG_SIZE):
        self.img_dir = os.path.join(root, f"keras_png_slices_{split}")
        self.msk_dir = os.path.join(root, f"keras_png_slices_seg_{split}")
        assert os.path.isdir(self.img_dir), f"missing {self.img_dir}"
        assert os.path.isdir(self.msk_dir), f"missing {self.msk_dir}"

        self.size = img_size
        self.imgs = sorted(glob.glob(os.path.join(self.img_dir, "*.nii.png")))
        assert len(self.imgs) > 0, "No images found (*.nii.png)"

        # Pair masks reliably
        self.msks = []
        misses = 0
        for ip in self.imgs:
            name = os.path.basename(ip)                     # case_001_slice_0.nii.png
            mname = name.replace("case_", "seg_")           # seg_001_slice_0.nii.png
            mp = os.path.join(self.msk_dir, mname)
            if not os.path.exists(mp):
                misses += 1
                mp = None
            self.msks.append(mp)
        assert misses == 0, f"{misses} masks missing; check zip integrity."

        # Detect classes from a sample of masks (use your discovered set)
        # We’ll also build a persistent remap LUT so CE expects 0..K-1
        vals = set()
        for p in self.msks[:50]:
            m = np.array(Image.open(p))
            vals |= set(np.unique(m))
        self.orig_vals = sorted(list(vals))          # e.g., [0, 85, 170, 255]
        self.num_classes = len(self.orig_vals)
        self.v2i = {v:i for i,v in enumerate(self.orig_vals)}  # map raw -> 0..K-1

    def __len__(self): return len(self.imgs)

    def _load_image(self, p):
        im = np.array(Image.open(p))
        if im.ndim == 2:
            im = im[..., None]
        im = normalize01(im)
        if self.size != im.shape[0]:  # resize only if needed
            im = np.array(Image.fromarray((im*255).astype(np.uint8)).resize(
                (self.size, self.size), Image.BILINEAR)) / 255.0
            if im.ndim == 2: im = im[..., None]
        return im

    def _load_mask(self, p):
        m = np.array(Image.open(p))
        if self.size != m.shape[0]:
            m = np.array(Image.fromarray(m).resize((self.size, self.size), Image.NEAREST))
        # remap labels to 0..K-1 using the detected palette
        m = np.vectorize(self.v2i.get)(m).astype(np.int64)
        return m

    def __getitem__(self, idx):
        img = self._load_image(self.imgs[idx])      # HWC in [0,1]
        msk = self._load_mask(self.msks[idx])       # HW  in {0..K-1}

        img = torch.from_numpy(img).permute(2,0,1).float()        # (C,H,W)
        ids = torch.from_numpy(msk).long()                         # (H,W)
        one_hot = F.one_hot(ids, num_classes=self.num_classes).permute(2,0,1).float()  # (C,H,W)
        return img, ids, one_hot


BASE = "/content/oasis_png/keras_png_slices_data"

train_set = OasisSegDataset("train", BASE, IMG_SIZE)
val_set   = OasisSegDataset("validate", BASE, IMG_SIZE)
test_set  = OasisSegDataset("test", BASE, IMG_SIZE)

print("Train/Val/Test:", len(train_set), len(val_set), len(test_set))
print("Classes:", train_set.num_classes, "palette:", train_set.orig_vals)

import os, glob
import numpy as np
from PIL import Image
import torch
from torch.utils.data import Dataset
import torch.nn.functional as F

IMG_SIZE = 256  # keep native

def normalize01(x):
    x = x.astype(np.float32)
    if x.max() > 1: x /= 255.0
    return x

class OasisSegDataset(Dataset):
    """
    Images: keras_png_slices_{split}/case_###_slice_#.nii.png  (or plain .png in some zips)
    Masks : keras_png_slices_seg_{split}/seg_ ###_slice_#.nii.png (or .png)
    """
    def __init__(self, split, root, img_size=IMG_SIZE):
        self.img_dir = os.path.join(root, f"keras_png_slices_{split}")
        self.msk_dir = os.path.join(root, f"keras_png_slices_seg_{split}")
        assert os.path.isdir(self.img_dir), f"missing {self.img_dir}"
        assert os.path.isdir(self.msk_dir), f"missing {self.msk_dir}"

        self.size = img_size
        # accept either .nii.png or .png for images
        imgs_nii = glob.glob(os.path.join(self.img_dir, "*.nii.png"))
        imgs_png = glob.glob(os.path.join(self.img_dir, "*.png"))
        self.imgs = sorted(set(imgs_nii) | set(imgs_png))
        assert len(self.imgs) > 0, "No images found"

        self.msks = []
        missing = []
        for ip in self.imgs:
            name = os.path.basename(ip)  # e.g. case_001_slice_0.nii.png
            base = name.replace("case_", "seg_")
            # try candidates without creating double “.nii”
            cands = []
            if base.endswith(".nii.png"):
                cands.append(base)                           # seg_... .nii.png
                cands.append(base.replace(".nii.png",".png"))# seg_... .png
            elif base.endswith(".png"):
                cands.append(base.replace(".png", ".nii.png"))
                cands.append(base)                           # seg_... .png
            else:
                cands.append(base + ".nii.png")
                cands.append(base + ".png")

            mpath = None
            for c in cands:
                trial = os.path.join(self.msk_dir, c)
                if os.path.exists(trial):
                    mpath = trial; break
            if mpath is None:
                missing.append((ip, cands))
            self.msks.append(mpath)

        assert len(missing) == 0, f"{len(missing)} masks missing, first few: {missing[:3]}"

        # detect label palette
        vals = set()
        for p in self.msks[:50]:
            m = np.array(Image.open(p))
            vals |= set(np.unique(m))
        self.orig_vals = sorted(list(vals))      # e.g. [0, 85, 170, 255]
        self.num_classes = len(self.orig_vals)
        self.v2i = {v:i for i,v in enumerate(self.orig_vals)}
        assert self.num_classes >= 2, "Unexpected num_classes; check masks."

    def __len__(self): return len(self.imgs)

    def _load_image(self, p):
        im = np.array(Image.open(p))
        if im.ndim == 2: im = im[..., None]
        im = normalize01(im)
        if self.size != im.shape[0]:
            im = np.array(Image.fromarray((im*255).astype(np.uint8)).resize((self.size, self.size), Image.BILINEAR))/255.0
            if im.ndim == 2: im = im[..., None]
        return im

    def _load_mask(self, p):
        m = np.array(Image.open(p))
        if self.size != m.shape[0]:
            m = np.array(Image.fromarray(m).resize((self.size, self.size), Image.NEAREST))
        m = np.vectorize(self.v2i.get)(m).astype(np.int64)  # remap to 0..K-1
        return m

    def __getitem__(self, idx):
        img = self._load_image(self.imgs[idx])     # HWC in [0,1]
        msk = self._load_mask(self.msks[idx])      # HW in {0..K-1}
        img = torch.from_numpy(img).permute(2,0,1).float()   # (C,H,W)
        ids = torch.from_numpy(msk).long()                   # (H,W)
        one_hot = F.one_hot(ids, num_classes=self.num_classes).permute(2,0,1).float()
        return img, ids, one_hot


BASE = "/content/oasis_png/keras_png_slices_data"
train_set = OasisSegDataset("train", BASE, IMG_SIZE)
val_set   = OasisSegDataset("validate", BASE, IMG_SIZE)
test_set  = OasisSegDataset("test", BASE, IMG_SIZE)

print("Classes:", train_set.num_classes, "palette:", train_set.orig_vals)

# set workers back to >0 if you like; keep 0 until we're sure
test_loader  = DataLoader(test_set, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)

test_pc, test_mean = eval_epoch(test_loader)
print("TEST Dice (per class):", np.round(test_pc, 4))
print("TEST Dice (mean):", test_mean)

# overlays
import os, matplotlib.pyplot as plt
os.makedirs("/content/seg_examples", exist_ok=True)
model.eval()
with torch.no_grad():
    saved = 0
    for img, ids, _ in test_loader:
        img, ids = img.to(device), ids.to(device)
        pred = model(img).softmax(1).argmax(1)
        for i in range(min(3, img.size(0))):
            im = img[i,0].detach().cpu().numpy()
            gt = ids[i].detach().cpu().numpy()
            pd = pred[i].detach().cpu().numpy()
            fig, ax = plt.subplots(1,3, figsize=(9,3))
            ax[0].imshow(im, cmap='gray'); ax[0].set_title('Image'); ax[0].axis('off')
            ax[1].imshow(gt, cmap='tab20'); ax[1].set_title('GT'); ax[1].axis('off')
            ax[2].imshow(pd, cmap='tab20'); ax[2].set_title('Pred'); ax[2].axis('off')
            fig.tight_layout()
            fig.savefig(f"/content/seg_examples/example_{saved}.png", dpi=150)
            plt.close(fig)
            saved += 1
        if saved >= 6: break

!ls -lh /content/seg_examples | sed -n '1,20p'


# === FINAL EVIDENCE CELL ===
import os, glob, numpy as np, matplotlib.pyplot as plt, pandas as pd, torch
from PIL import Image

# 1) Load weights (weights-only file is easiest)
WEIGHTS = "/content/best_unet_weights.pth"
assert os.path.exists(WEIGHTS), f"Missing {WEIGHTS}"
state = torch.load(WEIGHTS, map_location='cuda' if torch.cuda.is_available() else 'cpu')
model.load_state_dict(state)
model.eval()

# 2) Evaluate on test set (per-class & mean Dice)
test_pc, test_mean = eval_epoch(test_loader)
print("TEST Dice (per class):", np.round(test_pc, 4))
print("TEST Dice (mean):", round(test_mean, 4))

# Save metrics for your report/repo
os.makedirs("/content/reports", exist_ok=True)
df = pd.DataFrame({"class_id": list(range(len(test_pc))), "dice": test_pc})
df.loc[len(df)] = ["mean", test_mean]
df.to_csv("/content/reports/test_dice_metrics.csv", index=False)
print("Saved metrics -> /content/reports/test_dice_metrics.csv")

# 3) Make fresh evidence images (Image | GT | Pred) and show a grid
os.makedirs("/content/seg_examples", exist_ok=True)

def make_examples(max_images=6):
    saved = 0
    with torch.no_grad():
        for img, ids, _ in test_loader:
            img, ids = img.to(device), ids.to(device)
            pred = model(img).softmax(1).argmax(1)
            for i in range(img.size(0)):
                im = img[i,0].detach().cpu().numpy()
                gt = ids[i].detach().cpu().numpy()
                pd = pred[i].detach().cpu().numpy()
                fig, ax = plt.subplots(1,3, figsize=(9,3))
                ax[0].imshow(im, cmap='gray'); ax[0].set_title('Image'); ax[0].axis('off')
                ax[1].imshow(gt, cmap='tab20'); ax[1].set_title('GT'); ax[1].axis('off')
                ax[2].imshow(pd, cmap='tab20'); ax[2].set_title('Pred'); ax[2].axis('off')
                fig.tight_layout()
                outp = f"/content/seg_examples/example_{saved}.png"
                fig.savefig(outp, dpi=150)
                plt.close(fig)
                saved += 1
                if saved >= max_images: return

make_examples(max_images=6)
print("Saved examples -> /content/seg_examples")

# 4) Display a gallery (3 rows × 2 cols) inline for the demo
paths = sorted(glob.glob("/content/seg_examples/example_*.png"))[:6]
cols, rows = 2, int(np.ceil(len(paths)/2))
plt.figure(figsize=(10, 3*rows))
for i, p in enumerate(paths, 1):
    img = np.array(Image.open(p))
    plt.subplot(rows, cols, i); plt.imshow(img); plt.axis('off'); plt.title(os.path.basename(p))
plt.tight_layout(); plt.show()

# 5) (Optional) copy to Drive for submission
SUBMIT_DIR = "/content/drive/MyDrive/comp3710_unet_submit"
os.makedirs(SUBMIT_DIR, exist_ok=True)
!cp -f /content/reports/test_dice_metrics.csv "$SUBMIT_DIR/"
!cp -f /content/best_unet_weights.pth "$SUBMIT_DIR/"
!cp -rf /content/seg_examples "$SUBMIT_DIR/"
print("Copied results to:", SUBMIT_DIR)


